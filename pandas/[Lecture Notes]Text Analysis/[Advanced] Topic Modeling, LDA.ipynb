{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuOCC1wcBFfY"
   },
   "source": [
    "# 토픽모델링을 통한 키워드 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adQquisFBFfo"
   },
   "source": [
    "## 1. 사용할 라이브러리/데이터 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dyi0mnaFBFfs"
   },
   "outputs": [],
   "source": [
    "!pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icuWN--BBFfv"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.notebook import tqdm  # progress bar\n",
    "from konlpy.tag import Mecab; mecab = Mecab()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import warnings\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "%matplotlib inline\n",
    "\n",
    "## 운영체제별 글꼴 세팅\n",
    "\n",
    "# 그래프를 이쁘게 그리기 위한 코드입니다. 한글 글꼴을 추가합니다.\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "import matplotlib as mpl  # 기본 설정 만지는 용도\n",
    "import matplotlib.pyplot as plt  # 그래프 그리는 용도\n",
    "import matplotlib.font_manager as fm  # 폰트 관련 용도\n",
    "import seaborn as sns\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "sys_font=fm.findSystemFonts()\n",
    "print(f\"sys_font number: {len(sys_font)}\")\n",
    "print(sys_font)\n",
    "\n",
    "nanum_font = [f for f in sys_font if 'Nanum' in f]\n",
    "print(f\"nanum_font number: {len(nanum_font)}\")\n",
    "\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-nanum* -qq\n",
    "\n",
    "path = '/usr/share/fonts/truetype/nanum/NanumBarunGothicBold.ttf'  # 설치된 나눔글꼴중 원하는 녀석의 전체 경로를 가져옵니다.\n",
    "font_name = fm.FontProperties(fname=path, size=10).get_name()\n",
    "print(font_name)\n",
    "plt.rc('font', family=font_name)\n",
    "\n",
    "fm._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brEQeLSiBFfw"
   },
   "outputs": [],
   "source": [
    "# Colab Notebooks\n",
    "base_path = \"/content/drive/MyDrive/Colab Notebooks/data/topic_modeling/\"\n",
    "\n",
    "# 아까 크롤링한 파일\n",
    "## 크롤링한 파일이 바뀌면, 경로도 수정되어야 합니다.\n",
    "data_path = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwY3qvYXBFfx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "documents = \n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTjrMnLeBFfy"
   },
   "source": [
    "## 2. 전처리 함수 정의하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLX7qkcaBFfy"
   },
   "outputs": [],
   "source": [
    "def text_cleaning(doc):\n",
    "    # 한국어를 제외한 글자를 제거하는 패턴.\n",
    "    #doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
    "    \n",
    "    # 특수문자를 제거하는 패턴.\n",
    "    doc = re.sub(\"[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]\", \" \", doc)\n",
    "    \n",
    "    # 영문 빼고 모두 제거하는 패턴.\n",
    "    #doc = doc.replace(\"\\n\", \" \")\n",
    "    #doc = re.sub(\"[^A-Za-z ]\", \"\", doc)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def define_stopwords(path):\n",
    "    \n",
    "    SW = set()\n",
    "    # 불용어를 추가하는 방법 1.\n",
    "    # SW.add(\"있다\")\n",
    "    SW.add(\"있어요\")\n",
    "    \n",
    "    # 불용어를 추가하는 방법 2.\n",
    "    # stopwords-ko.txt에 직접 추가\n",
    "    \n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for word in f:\n",
    "            SW.add(word.strip())\n",
    "            \n",
    "    return SW\n",
    "\n",
    "def text_tokenizing(doc, tokenizer):\n",
    "    \"\"\"\n",
    "    Input Parameter :\n",
    "    \n",
    "    doc - tokenizing 하는 실제 데이터.\n",
    "    tokenizer - token의 단위.\n",
    "    \"\"\"\n",
    "    # 불용어(SW)에 포함되어 있지 않고, 두 글자 이상인 token(형태소)만 사용하겠다.\n",
    "    \n",
    "    if tokenizer == \"words\":\n",
    "        return [token for token in doc.split() if (token not in SW) and (len(token) >= 2)]\n",
    "    \n",
    "    elif tokenizer == \"nouns\":\n",
    "        return [token for token in mecab.nouns(doc) if (token not in SW) and (len(token) >= 2)] # 주어진 텍스트에서 명사만 뽑아서 리스트로 반환.\n",
    "        \n",
    "    elif tokenizer == \"morphs\":\n",
    "        return [token for token in mecab.morphs(doc) if (token not in SW) and (len(token) >= 2)] # 주어진 텍스트에서 형태소 단위로 잘라서 리스트로 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbaN1xzxBFfz"
   },
   "outputs": [],
   "source": [
    "tokenizer = \n",
    "SW = define_stopwords(base_path + \"stopwords-ko.txt\")\n",
    "tokenized_documents = [text_tokenizing(text_cleaning(doc), tokenizer=tokenizer) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mOJFy_oBFf1"
   },
   "outputs": [],
   "source": [
    "print(len(tokenized_documents))\n",
    "print(tokenized_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEWywlm6BFf2"
   },
   "source": [
    "### gensim LDA model을 사용하기 위한 자료구조 생성. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhcOnwKEBFf2"
   },
   "outputs": [],
   "source": [
    "# 문서-단어 행렬 만들기\n",
    "# 어휘(vocabulary) 학습\n",
    "dictionary = corpora.Dictionary(tokenized_documents)\n",
    "# 문서-단어 행렬 생성\n",
    "corpus = [dictionary.doc2bow(document) for document in tokenized_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6PVEzU0BFf3"
   },
   "outputs": [],
   "source": [
    "NUM_TOTAL_WORDS = len(dictionary)\n",
    "print(dictionary)\n",
    "print(NUM_TOTAL_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vpK1EQjBFgB"
   },
   "source": [
    "## 3. 토픽 모델링(LDA Model)\n",
    "\n",
    "\n",
    "- LDA(Latent Dirichlet Allocation) : 단어들의 조합(토픽)이 하나의 토픽을 구성하고, 각 단어가 그 토픽에 할당될 확률을 계산합니다.\n",
    "\n",
    "- LDA는 사람이 글을 쓰는 과정을 다음과 같이 가정합니다.\n",
    "\n",
    "1) 여러 개의 토픽을 정한다.\n",
    "\n",
    "2) 토픽 하나를 고릅니다.\n",
    "\n",
    "3) 토픽 내에 속하는 단어 하나를 고릅니다.\n",
    "\n",
    "4) 해당 단어를 글에 적습니다.\n",
    "\n",
    "5) 2번 과정부터 반복하면서 글을 적습니다.\n",
    "\n",
    "\n",
    "-> LDA는 사람이 글을 쓰는 과정을 따라하면서 글을 생성하는 과정을 학습합니다. (Generative Model)\n",
    "\n",
    "-> LDA는 생성을 통해서, 저자가 생각한 토픽의 분포(단어들의 확률 분포)를 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeuCGpTDBFgB"
   },
   "outputs": [],
   "source": [
    "# ldamodel 선언\n",
    "\n",
    "\n",
    "# Coherence model 선언 (LDA의 정량 평가 지표.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqy2CfaTBFgG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 토픽 개수와 토픽 별 상위 추출 단어 개수 지정.\n",
    "\n",
    "def build_doc_term_mat(documents):\n",
    "    \"\"\"주어진 문서 집합으로 문서-어휘 행렬을 만들어 돌려준다.\"\"\"\n",
    "    \n",
    "    print_log_msg(\"Building document-term matrix.\")\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    corpus = [dictionary.doc2bow(document) for document in documents]\n",
    "\n",
    "    return corpus, dictionary\n",
    "\n",
    "\n",
    "def print_topic_words(model):\n",
    "    \"\"\"토픽별 토픽 단어들을 화면에 인쇄한다.\"\"\"\n",
    "    \n",
    "    print_log_msg(\"Printing topic words.\")\n",
    "    \n",
    "    for topic_id in range(model.num_topics):\n",
    "        topic_word_probs = model.show_topic(topic_id, 30)\n",
    "        print(\"Topic ID: {}\".format(topic_id))\n",
    "\n",
    "        for topic_word, prob in topic_word_probs:\n",
    "            print(\"\\t{}\\t{}\".format(topic_word, prob))\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def print_log_msg(msg):\n",
    "    \"\"\"로그 메시지를 출력한다.\"\"\"\n",
    "    \n",
    "    print(msg, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f62R2rVNBFgH"
   },
   "outputs": [],
   "source": [
    "def compute_coherence(dictionary, corpus, texts, start, end, step):\n",
    "    '''\n",
    "    start, end+1, step에 해당하는 토픽 개수를 입력받아서 LdaModel을 수행하고, 그 때의 Coherence Score를 함께 돌려주는 함수.\n",
    "    '''\n",
    "    coherence_score_list = []\n",
    "    model_list = []\n",
    "    for num_topics in tqdm(range(start, end+1, step)):\n",
    "        model = \n",
    "        \n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=tokenized_documents, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score_list.append(coherence_model.get_coherence())\n",
    "        \n",
    "    return model_list, coherence_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXqQnYyEBFgI"
   },
   "outputs": [],
   "source": [
    "# LDA에서 가장 중요한 파라미터 = K(토픽 개수)\n",
    "start, end, step = [int(x) for x in input(\"원하는 토픽 갯수들을 입력하세요(e.g. 2,5,1)\").split(\",\")]\n",
    "start, end, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LVB_9ZJBFgJ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus, dictionary = build_doc_term_mat(tokenized_documents)\n",
    "print(len(corpus), len(dictionary))\n",
    "\n",
    "\n",
    "model_list, coherence_scores = compute_coherence(dictionary=dictionary, corpus=corpus, \n",
    "                                                texts=tokenized_documents, start=start, end=end, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tn-6rwwJBFgK"
   },
   "outputs": [],
   "source": [
    "coherence_list = coherence_scores\n",
    "label = \"Coherenece Score(C_V)\"\n",
    "\n",
    "x = range(start, end+1, step)\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.xticks(x)\n",
    "plt.plot(x, coherence_list, label=label)\n",
    "plt.scatter(x, coherence_list)\n",
    "plt.title(f\"Coherence Scores for LDA with {file_name}\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NY_UweJHBFgL"
   },
   "outputs": [],
   "source": [
    "selected_model = model_list[np.argmax(coherence_list)] # coherence score가 가장 높은 LDA model을 selected_model로 할당.\n",
    "selected_model.num_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQdp_zc9BFgO"
   },
   "source": [
    "### 4. pyLDAvis를 이용한 시각화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oVA5yjYod86"
   },
   "outputs": [],
   "source": [
    "!pip install pyldavis==3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rC3B54nNBFgP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pyLDAvis 불러오기\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# pyLDAvis를 jupyter notebook에서 실행할 수 있게 활성화.\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# pyLDAvis 실행.\n",
    "data = pyLDAvis.gensim.prepare()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQQy6TBsBFgQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1qvbPcG3_79Vgn8w3ZJc5XRlYyE35OKFs",
     "timestamp": 1662457047487
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "latex_metadata": {
   "author": "이기황",
   "coursetitle": "텍스트분석기법",
   "courseyear": "2018",
   "date": "2018.04.18",
   "logofile": "figs/ewhauniv-logo.png",
   "logoraise": "-.2",
   "logoscale": ".4",
   "title": "단어 임베딩과 토픽 모델링"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
