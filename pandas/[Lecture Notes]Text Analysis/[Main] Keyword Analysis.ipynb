{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 세팅 - 함수 정의 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 설치 및 로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import Mecab; mecab = Mecab()\n",
    "# 워드클라우드 관련\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "# 자연어처리 관련\n",
    "import nltk\n",
    "# 파일 처리 관련\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 함수 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_POS = [\"Noun\", \"Verb\", \"Adverb\", \"Adjective\"]\n",
    "\n",
    "\n",
    "def text_cleaning(doc):\n",
    "    # 한국어를 제외한 글자를 제거하는 패턴.\n",
    "    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
    "    \n",
    "    # 특수문자를 제거하는 패턴.\n",
    "    #doc = re.sub(\"[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]\", \" \", doc)\n",
    "    \n",
    "    # 영문 빼고 모두 제거하는 패턴.\n",
    "    #doc = doc.replace(\"\\n\", \" \")\n",
    "    #doc = re.sub(\"[^A-Za-z ]\", \"\", doc)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def define_stopwords(path):\n",
    "    \n",
    "    SW = set()\n",
    "    # 불용어를 추가하는 방법 1.\n",
    "    # SW.add(\"있다\")\n",
    "    SW.add(\"있어요\")\n",
    "    SW.add(\"대한\")\n",
    "    SW.add(\"합니다\")\n",
    "    SW.add(\"하는\")\n",
    "    \n",
    "    # 불용어를 추가하는 방법 2.\n",
    "    # stopwords-ko.txt에 직접 추가\n",
    "    \n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for word in f:\n",
    "            SW.add(word.strip())\n",
    "            \n",
    "    return SW\n",
    "\n",
    "def text_tokenizing(doc, tokenizer):\n",
    "    \"\"\"\n",
    "    Input Parameter :\n",
    "    \n",
    "    doc - tokenizing 하는 실제 데이터.\n",
    "    tokenizer - token의 단위.\n",
    "    \"\"\"\n",
    "    \n",
    "    if tokenizer == \"words\":\n",
    "        return [word for word in doc.split() if word not in SW and len(word) > 1]\n",
    "    \n",
    "    elif tokenizer == \"nouns\":\n",
    "        \n",
    "        \n",
    "    elif tokenizer == \"morphs\":\n",
    "        \n",
    "    \n",
    "    \n",
    "    elif tokenizer == \"predefined\":\n",
    "        \n",
    "        documents = []\n",
    "        text_pos = [pair for pair in okt.pos(doc) if pair[0] not in SW and len(pair[0]) > 1]\n",
    "        words = []\n",
    "\n",
    "        for word, pos in text_pos:\n",
    "            if pos in FEATURE_POS:\n",
    "                words.append(word)\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. 만약에 MeCab을 사용하려면 어떻게 해야할까?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer 지정하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = \"nouns\" # \"words\" / \"nouns\" / \"morphs\" / \"predefined\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing (cleansing, tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SW = define_stopwords(\"data/stopwords-ko.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_documents = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 빈도 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pprint import pprint\n",
    "\n",
    "total_tokens = \n",
    "print(len(total_tokens))\n",
    "\n",
    "text = nltk.Text(total_tokens)\n",
    "print(len(set(text.tokens)))\n",
    "# top 10 뽑기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도 출력 갯수.\n",
    "from nltk import FreqDist\n",
    "\n",
    "topN = 50\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "wordInfo = dict()\n",
    "\n",
    "for vocab, count in text.vocab().most_common(topN):\n",
    "    x.append(vocab)\n",
    "    y.append(count)\n",
    "    wordInfo[vocab] = count\n",
    "    \n",
    "test = {\"Counts\" : x, \"Samples\" : y}\n",
    "print(x[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈도 분석 히스토그램 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "%matplotlib inline\n",
    "\n",
    "# 글꼴 설정\n",
    "\n",
    "path = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc('font', family=font_name)\n",
    "else:\n",
    "    font_name = font_manager.FontProperties(fname=\"/usr/share/fonts/nanumfont/NanumGothic.ttf\")\n",
    "    rc('font', family=\"NanumGothic\")\n",
    "\n",
    "plt.figure(figsize=(16, 24))\n",
    "plt.title(\"네이버 뉴스 상위 %d 빈도 분석 히스토그램\" % (topN) , fontsize=18)\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Counts\")\n",
    "sns.barplot(x=\"Samples\", y=\"Counts\", data=test, palette=\"rainbow\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈도 분석 워드클라우드 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드클라우드에 들어갈 단어 갯수 변경.\n",
    "topN = 100\n",
    "\n",
    "# for windows : font_path='c:/Windows/Fonts/malgun.ttf'\n",
    "# for macOS : font_path = \"/usr/share/fonts/nanumfont/NanumGothic.ttf\"\n",
    "\n",
    "# 워드클라우드 만들 배경 그림 경로. ex) cloud.png\n",
    "\n",
    "mask = np.array(Image.open(\"data/cloud.png\"))\n",
    "wordcloud = WordCloud(font_path = \"malgun\",\n",
    "                      relative_scaling = 0.2,\n",
    "                      mask=mask,\n",
    "                      background_color='white',\n",
    "                      colormap=\"twilight\"\n",
    "                      ).generate_from_frequencies(wordInfo)\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"./wordcloud_naverNews(count).png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Term Frequency - Inverse Document Frequency(TF-IDF)는 단어의 절대 빈도와 문서간의 상대빈도 정보를 모두 활용하는 분석 방법입니다.\n",
    "\n",
    "\n",
    "\n",
    "* 단어가 무조건 많다고 좋은게 아니기 때문에, **특정 문서에만** 많은 단어를 주요 단어로 선정하고 싶습니다.\n",
    "\n",
    "\n",
    "\n",
    "* 그렇기에, 한 문서에는 많고 다른 문서에는 적은 단어를 계산을 통해 자동으로 찾아주는 방법을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 구하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "#TfidfVectorizer의 input으로 만들기 위한 전처리.\n",
    "tfidf_docs = \n",
    "\n",
    "tfidf = \n",
    "\n",
    "# tfidf 형태로 변환.\n",
    "X_tfidf = \n",
    "\n",
    "terms = tfidf.get_feature_names()\n",
    "\n",
    "# sum tfidf frequency of each term through documents\n",
    "sums = X_tfidf.sum(axis=0)\n",
    "\n",
    "# connecting term to its sums frequency\n",
    "df = []\n",
    "for col, term in enumerate(terms):\n",
    "    df.append( (term, sums[0,col] ))\n",
    "\n",
    "ranking = pd.DataFrame(df, columns=['Term','TF-IDF'])\n",
    "rankInfo = ranking.sort_values('TF-IDF', ascending=False)[:50]\n",
    "\n",
    "x = list(rankInfo[\"Term\"])\n",
    "y = list(rankInfo[\"TF-IDF\"])\n",
    "\n",
    "tfidfInfo = dict()\n",
    "\n",
    "for vocab, tfidf in zip(x, y):\n",
    "    tfidfInfo[vocab] = tfidf\n",
    "\n",
    "print(x[:5])\n",
    "print(y[:5])\n",
    "print()\n",
    "print(rankInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 히스토그램 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 24))\n",
    "plt.title(\"네이버 뉴스 상위 %d TF-IDF 분석 히스토그램\" % (topN) , fontsize=18)\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Counts\")\n",
    "sns.barplot(x=\"TF-IDF\", y=\"Term\", data=rankInfo, palette=\"rainbow\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 기준 워드클라우드 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 워드클라우드에 들어갈 단어 갯수 변경.\n",
    "\n",
    "# for windows : font_path='c:/Windows/Fonts/malgun.ttf'\n",
    "# for macOS : font_path = \"/usr/share/fonts/nanumfont/NanumGothic.ttf\"\n",
    "\n",
    "# 워드클라우드 만들 배경 그림 경로. ex) cloud.png\n",
    "\n",
    "mask = np.array(Image.open(\"data/cloud.png\"))\n",
    "wordcloud = WordCloud(font_path = \"malgun\",\n",
    "                      relative_scaling = 0.2,\n",
    "                      #stopwords=STOPWORDS,\n",
    "                      mask=mask,\n",
    "                      background_color='white',\n",
    "                      colormap=\"twilight\"\n",
    "                      ).generate_from_frequencies(tfidfInfo)\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"./wordcloud_naverNews(tfidf).png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TextRank 분석 (키워드 추출 알고리즘) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Main Concept : 중요한 단어와 함깨 등장하는 단어도 중요하다!\n",
    "\n",
    "\n",
    "\n",
    "* 문장 내에 같이 등장하는 단어(co-occurence) 정보를 반영하여, 중요단어를 선정함.\n",
    "\n",
    "\n",
    "\n",
    "* TF-IDF는 문서 내에 동시등장 정보를 활용하지 않기 때문에, 이를 활용하여 주요 단어를 선정하는 기법이 TextRank입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>Reference : https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/lovit/textrank.git  ## 맨처음에 한번만 실행시켜주세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank를 위한 tokenizing 함수 재정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(doc):\n",
    "    return [word for word in doc.split() if word not in SW and len(word) > 1]\n",
    "    \n",
    "def noun_tokenizer(doc):\n",
    "    return [word for word in okt.nouns(doc) if word not in SW and len(word) > 1]\n",
    "        \n",
    "def morph_tokenizer(doc):\n",
    "    return [word for word in okt.morphs(doc) if word not in SW and len(word) > 1]\n",
    "\n",
    "\n",
    "def predefined_tokenizer(doc):\n",
    "    text_pos = [pair for pair in okt.pos(doc) if pair[0] not in SW and len(pair[0]) > 1]\n",
    "    words = []\n",
    "\n",
    "    for word, pos in text_pos:\n",
    "        if pos in FEATURE_POS:\n",
    "            words.append(word)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textrank.textrank import KeywordSummarizer\n",
    "\n",
    "\n",
    "if tokenizer == \"words\":\n",
    "    tokenize_func = word_tokenizer\n",
    "\n",
    "elif tokenizer == \"nouns\":\n",
    "    tokenize_func = noun_tokenizer\n",
    "\n",
    "elif tokenizer == \"morphs\":\n",
    "    tokenize_func = morph_tokenizer\n",
    "    \n",
    "elif tokenizer == \"predefined\":\n",
    "    tokenize_func = predefined_tokenizer\n",
    "\n",
    "# TextRank 함수 정의\n",
    "keyword_extractor = KeywordSummarizer(\n",
    "    tokenize = ,\n",
    "    min_count=,                   # number of minimum appear\n",
    "    window=-1,                     # cooccurrence within a sentence\n",
    "    min_cooccurrence=2,\n",
    "    vocab_to_idx=None,             # you can specify vocabulary to build word graph\n",
    "    df=0.85,                       # PageRank damping factor\n",
    "    max_iter=100,                  # PageRank maximum iteration\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextRank 수행\n",
    "keywords = \n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "textRankInfo = dict()\n",
    "\n",
    "for vocab, score in keywords:\n",
    "    x.append(vocab)\n",
    "    y.append(score)\n",
    "    textRankInfo[vocab] = score\n",
    "    \n",
    "test = {\"TextRank Score\" : x, \"Term\" : y}\n",
    "print(x[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 24))\n",
    "plt.title(\"네이버 뉴스 상위 %d 키워드 분석 히스토그램\" % (topN) , fontsize=18)\n",
    "plt.xlabel(\"Term\")\n",
    "plt.ylabel(\"TextRank Score\")\n",
    "sns.barplot(x=\"Term\", y=\"TextRank Score\", data=test, palette=\"rainbow\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(Image.open(\"data/cloud.png\"))\n",
    "wordcloud = WordCloud(font_path = \"malgun\",\n",
    "                      relative_scaling = 0.2,\n",
    "                      #stopwords=STOPWORDS,\n",
    "                      mask=mask,\n",
    "                      background_color='white',\n",
    "                      colormap=\"twilight\"\n",
    "                      ).generate_from_frequencies(textRankInfo)\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"./wordcloud_NSMC(textRank).png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go Further? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. 어떤 작업들을 하면 더 좋은 결과를 볼 수 있을까요?**\n",
    "\n",
    "1. 단어? 명사? 형태소?\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "2. 텍스트 정규화 -> 비슷한 뜻을 가진 단어 합치기\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "\n",
    "3. 띄어쓰기는 멀쩡할까?\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "\n",
    "4. 딥러닝으로 해보면 어떨까? \n",
    "\n",
    "\n",
    "**Q3. TF-IDF와 TextRank에서 키워드의 순위가 차이가 나는 이유는 무엇일까?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
